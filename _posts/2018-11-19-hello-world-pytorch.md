---
layout:     post
title:      "Hello World Pytorch"
subtitle:   " \"The new stage of artificial intelligence\""
date:       2018-11-19 22:17:00
author:     "Nero"
toc:        true
header-img: "img/post-bg-ml.jpg"
tags:
    - å­¸ç¿’ç­†è¨˜
    - æ©Ÿå™¨å­¸ç¿’
    - Pytorch
---
> â€œåˆæ¢æ©Ÿå™¨å­¸ç¿’ï¼ŒPytorch åŸºç¤ï¼â€
{: .colorquote .info}

## ç°¡ä»‹
{:toc}

## Tensor
{:toc}
Tensor å³å¼µé‡è¡¨ç¤ºçš„æ˜¯ä¸€å€‹å¤šç¶­çš„çŸ©é™£ï¼Œé›¶ç‚ºæ˜¯ä¸€å€‹é»ï¼Œä¸€ç¶­æ˜¯å‘é‡ï¼ŒäºŒç¶­æ˜¯å¹³é¢çŸ©é™£ï¼Œå¤šç‚ºç›¸ç•¶é€£çºŒçš„æ•¸çµ„ã€‚
`torch.Tensor`æ˜¯ä¸€ä¸­åŒ…å«å–®ä¸€æ•¸æ“šé¡å‹çš„å¤šç¶­é™£åˆ—ï¼ŒTorchå®šç¾©äº†ä¸ƒç¨®CPUå¼µé‡é¡å‹å’Œå…«ç¨®GPUå¼µé‡é¡å‹ï¼Œ`torch.Tensor`é»˜èªçš„tensoré¡å‹ç‚ºï¼ˆtorch.FlaotTensorï¼‰ã€‚

æ•¸æ“štype       | CPUå¼µé‡ |GPUå¼µé‡ |
--------------|:------:|:------:|
32ä½æµ®é»	     | torch.FloatTensor   |torch.cuda.FloatTensor
64ä½æµ®é»        | torch.DoubleTensor  |torch.cuda.DoubleTensor
16ä½æµ®é»    	 | N / A       	       |torch.cuda.HalfTensor
8ä½æ•´æ•¸ï¼ˆç„¡ç¬¦è™Ÿï¼‰ |	torch.ByteTensor	|torch.cuda.ByteTensor
8ä½æ•´æ•¸ï¼ˆå¸¶ç¬¦è™Ÿï¼‰ |torch.CharTensor     |torch.cuda.CharTensor
16ä½æ•´æ•¸ï¼ˆå¸¶ç¬¦è™Ÿï¼‰|	torch.ShortTensor	|torch.cuda.ShortTensor
32ä½æ•´æ•¸ï¼ˆå¸¶ç¬¦è™Ÿï¼‰|	torch.IntTensor	    |torch.cuda.IntTensor
64ä½æ•´æ•¸ï¼ˆå¸¶ç¬¦è™Ÿï¼‰|	torch.LongTensor	|torch.cuda.LongTensor

ä»¥ä¸‹ç¯„ä¾‹ç‚ºåˆ©ç”¨ä¸€å€‹ python çš„ list å»ºç«‹ä¸€å€‹tensorã€‚
```
>>> torch.FloatTensor([[1, 2, 3], [4, 5, 6]])

1 2 3
4 5 6
[torch.FloatTensor of size 2x3]
```
ä¹Ÿå¯ä»¥å‰µå»ºä¸€å€‹å…§å®¹å…¨ç‚º0çš„ Tensor 
```
>>> torch.zeros((3, 2))

tensor([[0., 0.],
        [0., 0.],
        [0., 0.]])
```
æˆ–è€…äº‚æ•¸ç”¢ç”Ÿéš¨æ©Ÿåˆå§‹å€¼
```
>>> torch.randn((3, 2))

tensor([[ 1.8436, -0.2691],
        [ 0.5289, -1.6964],
        [-0.3884, -0.0773]])
```
å¯ä»¥é€é index æŒ‡å®šæ”¹è®Šçš„æ•¸å€¼
```
>>> a = torch.Tensor([[1, 2, 3], [4, 5, 6]])
>>> a[1, 2]=100
>>> print(a)

tensor([[  1.,   2.,   3.],
        [  4.,   5., 100.]])
```
torch.Tensor å¯è½‰æ›ç‚º numpy.ndarrayã€‚
```
>>> a = torch.FloatTensor([[1, 2, 3], [4, 5, 6]])
>>> numpy_a = a.numpy()
>>> print(numpy_a)

array([[1., 2., 3.],
       [4., 5., 6.]], dtype=float32)
```
ä¹Ÿå¯ä»¥å¾ numpy.ndarray è½‰æ›ç‚º torch.Tensorã€‚
```
>>> a = np.array([[2,3],[4,5]])
>>> torch_a = torch.from_numpy(a)
>>> print(torch_a)
```
è€Œéœ€è¦é€é GPU åŠ é€Ÿæ™‚åªéœ€è¦åŠ ä¸Š cuda()ã€‚
```
if torch.cuda.is_available():
    a_cuda = a.cuda()
    print(a_cuda)
```
## Variable
{:toc}
Variable æ˜¯ PyTorch ä¸­ä¸€å€‹é‡è¦çš„æ¦‚å¿µï¼ŒVariable æœ¬è³ªä¸Šèˆ‡ Tansor æ²’æœ‰å€åˆ¥ç”¨ä»¥ä¿å­˜æ•¸æ“šï¼Œå·®åˆ¥åœ¨æ–¼ Variable ç”¨ä»¥ä¿å­˜ç¥ç¶“ç¶²è·¯è¨“ç·´æœŸé–“è®ŠåŒ–çš„å€¼ï¼Œå³æˆ‘å€‘ç¶²è·¯å¯å­¸ç¿’å¾—åƒæ•¸ï¼Œè€Œ Tensor ä¸å„²å­˜è¨“ç·´éç¨‹ä¸­å­¸ç¿’çš„å€¼ã€‚
<p align="center">
  <img width="90%" height="90%" src="https://raw.githubusercontent.com/NeroCube/nerocube.github.io/master/img/in-post/2018-11-19-hello-world-pytorch/variable.png">
</p>
Variable é¡åˆ¥åŒ…å«äº† Tensor ï¼Œå…·æœ‰ä¸‹åˆ—å±¬æ€§ã€‚
- data: å¯é€é Variable.data å±¬æ€§ä¾†è¨ªå•å¼µé‡ã€‚
- grad: å¯é€é Variable.grad å±¬æ€§ä¾†è¨ªå•åå‘å‚³æ’­æ¢¯åº¦ã€‚
- grad_fn: è©²åƒæ•¸ç´€éŒ„è‘—å¾—åˆ°è‘—å€‹ Variable çš„æ“ä½œï¼Œä¾‹å¦‚æ˜¯é€éåŠ æ¸›æˆ–ä¹˜é™¤ã€‚

åœ¨çœŸå¯¦ä¸–ç•Œä¸­æˆ‘å€‘å¯ä»¥å°‡ä»»ä½•å•é¡Œè½‰æ›ç‚ºä¸€å€‹ function of x å³ ğ‘“(ğ‘¥)ï¼Œæ©Ÿå™¨å­¸ç¿’å‰‡æ˜¯å¸Œæœ›ç›¡é‡æ¸›å°‘é€™å€‹ function èˆ‡çœŸå¯¦ä¸–ç•Œçš„å·®è·ï¼Œåœ¨ç¶²è·¯è¨“ç·´çš„éç¨‹ä¸­ input ç¶“é `forward()` æœ€å¾Œè¼¸å‡º output ï¼Œå¯é€éè¨­å®š`requires_grad=True`åœ¨ value ç¶“é `forward()` æ™‚å‘ŠçŸ¥`backward()`åœ¨ç™¼ç”Ÿåå‘å‚³æ’­æ¢¯åº¦(backpropagation)æ™‚ä¾¿æœƒè‡ªå‹•è¨ˆç®— gradient é”åˆ° Autograd çš„æ•ˆæœã€‚

## Module
{:toc}
åœ¨ PyTorch ä¸­æ‰€æœ‰çš„ç¥ç¶“ç¶²è·¯éƒ½æ˜¯åŸºæ–¼ `torch.nn.Module` ï¼Œä¸” Module å¯ä»¥ä»¥æ¨¹ç‹€çµæ§‹å·¢ç‹€åŒ…å«å…¶å®ƒ Module ï¼Œä»¥ä¸‹ç‚ºä¸€å€‹å¦‚ä½•å»ºç«‹ Module çš„å¯¦éš›ç¯„ä¾‹ã€‚

```
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)# submodule: Conv2d
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
       x = F.relu(self.conv1(x))
       return F.relu(self.conv2(x))
```

## Optimizer
{:toc}
åœ¨æ©Ÿå™¨å­¸ç¿’ä¸­æˆ‘å€‘éœ€è¦é€éåœ¨èª¿æ•´åƒæ•¸ä¾†ä½¿ loss function æœ€å°åŒ–ï¼Œ Optimizer ç®—æ˜¯èª¿æ•´åƒæ•¸æ›´æ–°çš„ç­–ç•¥ï¼ŒPytorch ä¸­ `torch.optim` å¯¦ä½œäº†å„ç¨®å„ªåŒ–ç®—æ³•ï¼Œä¸‹åˆ—ç‚ºå¦‚ä½•åœ¨ PyTorch ä¸­å»ºç«‹ä¸€å€‹ Optimizerã€‚
```
optimizer = torch.optim.SDG(model.parameters(), lr=0.01, momentum = 0.9)
```
æˆ‘å€‘ä½¿ç”¨ `Stochastic Gradient Descent (SGD)` ä½œç‚ºæˆ‘å€‘å„ªåŒ–çš„ç­–ç•¥ä¸¦å¸¶å…¥å­¸ç¿’ç‡ ç‚º 0.01 èˆ‡å‹•é‡ 0.9 ï¼Œåœ¨æ¯å›æˆ‘å€‘éœ€è¦å°‡è¼¸å‡ºçš„æ•¸å€¼èˆ‡å¯¦éš›æ•¸å€¼å¸¶å…¥ `loss function` ç„¶å¾Œå°‡ optimizer çš„ gradient æ­¸é›¶ï¼Œæ¥è‘—é€é `loss.backward()` åå‘å‚³æ’­ç®—å‡º 
 gradient æ±ºå®šè¦èµ°çš„æ–¹å‘ï¼Œæœ€å¾Œåªéœ€è¦ `optimizer.step()` å°±å¯ä»¥é€é gradient æ›´æ–°åƒæ•¸ã€‚
```
loss = loss_func(output, target)   
optimizer.zero_grad()           
loss.backward()                 
optimizer.step()  
```
## åƒè€ƒè³‡æ–™
{:toc}

- [Getting Started with PyTorch Part 1: Understanding how Automatic Differentiation works](https://towardsdatascience.com/getting-started-with-pytorch-part-1-understanding-how-automatic-differentiation-works-5008282073ec)

- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)